{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {},
   "outputs": [],
   "source": [
    "import socket\n",
    "import struct\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from scipy.signal import stft\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from classifiers_linear import LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegTune(LogisticRegression):\n",
    "    def __init__(self,\n",
    "                 penalty='l1',\n",
    "                 *,\n",
    "                 dual=False,\n",
    "                 tol=0.0001,\n",
    "                 C=1.0,\n",
    "                 fit_intercept=True,\n",
    "                 intercept_scaling=1,\n",
    "                 class_weight=None,\n",
    "                 random_state=None,\n",
    "                 solver='liblinear',\n",
    "                 max_iter=100,\n",
    "                 multi_class='ovr',\n",
    "                 verbose=0,\n",
    "                 warm_start=False,\n",
    "                 n_jobs=None,\n",
    "                 l1_ratio=None,\n",
    "                 scaler=None,\n",
    "                 halfwin=7,\n",
    "                 overlap=2,\n",
    "                 num_channels=[0, 1, 2, 64, 65, 66]):\n",
    "        self.scaler = scaler\n",
    "        self.halfwin = halfwin\n",
    "        self.overlap = overlap\n",
    "        self.num_channels = num_channels\n",
    "        self.sr_data = 500\n",
    "        self.sample_rate = [0, 250]\n",
    "\n",
    "        super().__init__(penalty=penalty, C=C, solver=solver, multi_class=multi_class)\n",
    "\n",
    "    def fit_func(self, data, target):\n",
    "        super().fit(data, target)\n",
    "\n",
    "    def fit(self, data, target):\n",
    "        if self.scaler is None:\n",
    "            self.scaler_inst = FunctionTransformer(lambda x: x)\n",
    "        else:\n",
    "            self.scaler_inst = self.scaler()\n",
    "\n",
    "        num_chn = len(self.num_channels)\n",
    "\n",
    "        num_trials = data.shape[0]\n",
    "        data = data[:, ::int(1000 / self.sr_data), self.num_channels]\n",
    "        data = data[:, self.sample_rate[0]:self.sample_rate[1], :].reshape(-1, num_chn)\n",
    "\n",
    "        self.scaler_inst.fit(data)\n",
    "        \n",
    "        data = self.scaler_inst.transform(data)\n",
    "        data = data.reshape(num_trials, -1, num_chn)\n",
    "\n",
    "        args = Args()\n",
    "        args.sr_data = self.sr_data\n",
    "        args.halfwin = self.halfwin\n",
    "        args.overlap = self.overlap\n",
    "        data = wavelet_transform(data, args)\n",
    "        data = data.reshape(num_trials, -1)\n",
    "\n",
    "        # fit model\n",
    "        self.fit_func(data, target)\n",
    "\n",
    "    def predict_func(self, data):\n",
    "        return super().predict(data)\n",
    "\n",
    "    def predict(self, data):\n",
    "        num_chn = len(self.num_channels)\n",
    "\n",
    "        num_trials = data.shape[0]\n",
    "        data = data[:, ::int(1000 / self.sr_data), self.num_channels]\n",
    "        data = data[:, self.sample_rate[0]:self.sample_rate[1], :].reshape(-1, num_chn)\n",
    "\n",
    "        data = self.scaler_inst.transform(data)\n",
    "        data = data.reshape(num_trials, -1, num_chn)\n",
    "\n",
    "        args = Args()\n",
    "        args.sr_data = self.sr_data\n",
    "        args.halfwin = self.halfwin\n",
    "        args.overlap = self.overlap\n",
    "        data = wavelet_transform(data, args)\n",
    "        data = data.reshape(num_trials, -1)\n",
    "\n",
    "        # fit model\n",
    "        return self.predict_func(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LDATune(LinearDiscriminantAnalysis):\n",
    "    def __init__(self,\n",
    "                 solver='lsqr',\n",
    "                 shrinkage='auto',\n",
    "                 priors=None,\n",
    "                 n_components=None,\n",
    "                 store_covariance=False,\n",
    "                 tol=0.0001,\n",
    "                 covariance_estimator=None,\n",
    "                 pca_comps=1,\n",
    "                 scaler=None,\n",
    "                 halfwin=7,\n",
    "                 overlap=2,\n",
    "                 num_channels=[0, 1, 2, 64, 65, 66],\n",
    "                 wavelet=True):\n",
    "        self.scaler = scaler\n",
    "        self.halfwin = halfwin\n",
    "        self.overlap = overlap\n",
    "        self.num_channels = num_channels\n",
    "        self.pca_comps = pca_comps\n",
    "        self.sr_data = 500\n",
    "        self.sample_rate = [0, 250]\n",
    "        self.wavelet = wavelet\n",
    "\n",
    "        super().__init__(solver=solver, shrinkage=shrinkage)\n",
    "\n",
    "    def fit(self, data, target):\n",
    "        if self.scaler is None:\n",
    "            self.scaler_inst = FunctionTransformer(lambda x: x)\n",
    "        else:\n",
    "            self.scaler_inst = self.scaler()\n",
    "\n",
    "        num_chn = len(self.num_channels)\n",
    "\n",
    "        num_trials = data.shape[0]\n",
    "        data = data[:, ::int(1000 / self.sr_data), self.num_channels]\n",
    "        data = data[:, self.sample_rate[0]:self.sample_rate[1], :].reshape(-1, num_chn)\n",
    "\n",
    "        self.scaler_inst.fit(data)\n",
    "        \n",
    "        data = self.scaler_inst.transform(data)\n",
    "        data = data.reshape(num_trials, -1, num_chn)\n",
    "\n",
    "        args = Args()\n",
    "        args.sr_data = self.sr_data\n",
    "        args.halfwin = self.halfwin\n",
    "        args.overlap = self.overlap\n",
    "        if self.wavelet:\n",
    "            data = wavelet_transform(data, args)\n",
    "        data = data.reshape(num_trials, -1)\n",
    "\n",
    "        self.pca = PCA(n_components=self.pca_comps)\n",
    "        data = self.pca.fit_transform(data)\n",
    "\n",
    "        # fit model\n",
    "        super().fit(data, target)\n",
    "\n",
    "    def predict(self, data):\n",
    "        num_chn = len(self.num_channels)\n",
    "\n",
    "        num_trials = data.shape[0]\n",
    "        data = data[:, ::int(1000 / self.sr_data), self.num_channels]\n",
    "        data = data[:, self.sample_rate[0]:self.sample_rate[1], :].reshape(-1, num_chn)\n",
    "\n",
    "        data = self.scaler_inst.transform(data)\n",
    "        data = data.reshape(num_trials, -1, num_chn)\n",
    "\n",
    "        args = Args()\n",
    "        args.sr_data = self.sr_data\n",
    "        args.halfwin = self.halfwin\n",
    "        args.overlap = self.overlap\n",
    "        if self.wavelet:\n",
    "            data = wavelet_transform(data, args)\n",
    "        data = data.reshape(num_trials, -1)\n",
    "\n",
    "        data = self.pca.transform(data)\n",
    "\n",
    "        # fit model\n",
    "        return super().predict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegL1(LDA):\n",
    "    '''\n",
    "    Logistic Regression model using the functionalities of the LDA class.\n",
    "    Uses L1 regularization.\n",
    "    '''\n",
    "    def __init__(self, args):\n",
    "        self.model = LogisticRegression(multi_class='ovr',\n",
    "                                        penalty='l1',\n",
    "                                        solver='liblinear',\n",
    "                                        C=args.C_reg)\n",
    "        self.fit_pca = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    def __init__(self):\n",
    "        n = 1  # can be used to do multiple runs, e.g. over subjects\n",
    "\n",
    "        self.model = LogisticRegL1\n",
    "        self.scaler = StandardScaler\n",
    "        self.C_reg = 1\n",
    "        self.sample_rate = [0, 250]\n",
    "        self.num_channels = [2, 64]  # 2, 64\n",
    "        self.sr_data = 500  # sampling rate used for downsampling\n",
    "        self.streaming_SR = 5\n",
    "        self.halfwin = 15  # 10\n",
    "        self.overlap = 2 # 2\n",
    "        self.result_dir = os.path.join(\n",
    "            '..',  # path(s) to save model and others\n",
    "            'results',\n",
    "            'stream_jaw_6chan')\n",
    "\n",
    "        # experiment arguments\n",
    "        self.name = 'args.py'  # name of this file, don't change\n",
    "        self.fix_seed = False\n",
    "        self.common_dataset = False\n",
    "        self.load_dataset = True  # whether to load self.dataset\n",
    "        self.learning_rate = 0.0001  # learning rate for Adam\n",
    "        self.max_trials = 1  # ratio of training data (1=max)\n",
    "        self.val_max_trials = False\n",
    "        self.batch_size = 20  # batch size for training and validation data\n",
    "        self.epochs = 5000  # number of loops over training data\n",
    "        self.val_freq = 20  # how often to validate (in epochs)\n",
    "        self.print_freq = 5  # how often to print metrics (in epochs)\n",
    "        self.save_curves = True  # whether to save loss curves to file\n",
    "        self.load_model = False  # class of model to use\n",
    "        self.dataset = None  # dataset class for loading and handling data\n",
    "\n",
    "        # wavenet arguments\n",
    "        self.activation = None  # activation function for models\n",
    "        self.subjects = 1  # number of subjects used for training\n",
    "        self.embedding_dim = 0  # subject embedding size\n",
    "        self.p_drop = 0.6  # dropout probability\n",
    "        self.ch_mult = 2  # channel multiplier for hidden channels in wavenet\n",
    "        self.kernel_size = 2  # convolutional kernel size\n",
    "        self.timesteps = 1  # how many timesteps in the future to forecast\n",
    "        self.rf = 256  # receptive field of wavenet\n",
    "        rf = 128\n",
    "        ks = self.kernel_size\n",
    "        nl = int(np.log(rf) / np.log(ks))\n",
    "        dilations = [ks**i for i in range(nl)]\n",
    "        self.dilations = dilations + dilations   # dilation: 2^num_layers\n",
    "        #self.dilations = [1] + [2] + [4] * 7  # costum dilations\n",
    "\n",
    "        # classifier arguments\n",
    "        self.wavenet_class = None  # class of wavenet model\n",
    "        self.load_conv = 'y'  # where to load neural nerwork weights from\n",
    "        self.pred = False  # whether to use wavenet in prediction mode\n",
    "        self.init_model = True  # whether to reinitialize classifier\n",
    "        self.reg_semb = True  # whether to regularize subject embedding\n",
    "        self.fixed_wavenet = False  # whether to fix weights of wavenet\n",
    "        self.alpha_norm = 0.0  # regularization multiplier on weights\n",
    "        self.num_classes = 118  # number of classes for classification\n",
    "        self.units = [2200, 2000]  # hidden layer sizes of fully-connected block\n",
    "        self.dim_red = 80  # number of pca components for channel reduction\n",
    "        self.stft_freq = 0  # STFT frequency index for LDA_wavelet_freq model\n",
    "        self.decode_peak = 0.1\n",
    "        self.trial_average = False\n",
    "\n",
    "        # quantized wavenet arguments\n",
    "        self.mu = 255\n",
    "        self.residual_channels = 1024\n",
    "        self.dilation_channels = 1024\n",
    "        self.skip_channels = 1024\n",
    "        self.class_emb = 10\n",
    "        self.channel_emb = 30\n",
    "        self.cond_channels = self.class_emb + self.channel_emb\n",
    "        self.head_channels = int(self.skip_channels/2)\n",
    "        self.conv_bias = False\n",
    "\n",
    "        # dataset arguments\n",
    "        data_path = os.path.join('/', 'gpfs2', 'well', 'woolrich', 'projects',\n",
    "                                 'cichy118_cont', 'preproc_data_onepass', 'epoched')\n",
    "        self.data_path = [os.path.join(data_path, f'subj{i}') for i in range(n)]  # path(s) to data directory\n",
    "        self.numpy = True  # whether data is saved in numpy format\n",
    "        self.crop = 1  # cropping ratio for trials\n",
    "        self.shuffle = True\n",
    "        self.whiten = False  # pca components used in whitening\n",
    "        self.group_whiten = False  # whether to perform whitening at the GL\n",
    "        self.split = np.array([0, 0.2])  # validation split (start, end)\n",
    "        self.original_sr = 1000\n",
    "        self.save_data = True  # whether to save the created data\n",
    "        self.save_whiten = False\n",
    "        self.subjects_data = False  # list of subject inds to use in group data\n",
    "        self.num_clip = 25\n",
    "        self.dump_data = [os.path.join(data_path, f'subj{i}', 'train_data_trialnorm', 'c') for i in range(n)]  # path(s) for dumping data\n",
    "        self.load_data = self.dump_data  # path(s) for loading data files\n",
    "\n",
    "        # analysis arguments\n",
    "        self.kernelPFI = False\n",
    "        self.closest_chs = 'notebooks/closest1'  # channel neighbourhood size for spatial PFI\n",
    "        self.PFI_inverse = False  # invert which channels/timesteps to shuffle\n",
    "        self.pfich_timesteps = [[0, 50]]  # time window for spatiotemporal PFI\n",
    "        self.PFI_perms = 10  # number of PFI permutations\n",
    "        self.halfwin_uneven = False  # whether to use even or uneven window\n",
    "        self.generate_noise = 1  # noise used for wavenet generation\n",
    "        self.generate_length = self.sr_data * 1000  # generated timeseries len\n",
    "        self.generate_mode = 'IIR'  # IIR or FIR mode for wavenet generation\n",
    "        self.generate_input = 'gaussian_noise'  # input type for generation\n",
    "        self.individual = True  # whether to analyse individual kernels\n",
    "        self.anal_lr = 0.001  # learning rate for input backpropagation\n",
    "        self.anal_epochs = 200  # number of epochs for input backpropagation\n",
    "        self.norm_coeff = 0.0001  # L2 of input for input backpropagation\n",
    "        self.kernel_limit = 300  # max number of kernels to analyse\n",
    "\n",
    "        # simulation arguments\n",
    "        self.nonlinear_prenoise = True\n",
    "        self.nonlinear_data = True\n",
    "        self.seconds = 3000\n",
    "        self.events = 8\n",
    "        self.sim_num_channels = 1\n",
    "        self.sim_ar_order = 2\n",
    "        self.gamma_shape = 14\n",
    "        self.gamma_scale = 14\n",
    "        self.noise_std = 2.5\n",
    "        self.lambda_exp = 0.005\n",
    "        self.ar_shrink = 1.0\n",
    "        self.freqs = []\n",
    "        self.ar_noise_std = np.random.rand(self.events) / 5 + 0.8\n",
    "        self.max_len = 1000\n",
    "\n",
    "        # AR model arguments\n",
    "        self.order = 64\n",
    "        self.uni = False\n",
    "        self.save_AR = False\n",
    "        self.do_anal = False\n",
    "        self.AR_load_path = os.path.join(\n",
    "            'results',\n",
    "            'mrc',\n",
    "            '60subjects_notch_sensors_multiAR64')\n",
    "\n",
    "        # unused\n",
    "        self.num_plot = 1\n",
    "        self.plot_ch = 1\n",
    "        self.linear = False\n",
    "        self.num_samples_CPC = 20\n",
    "        self.dropout2d_bad = False\n",
    "        self.k_CPC = 1\n",
    "        self.groups = 1\n",
    "        self.conv1x1_groups = 1\n",
    "        self.pos_enc_type = 'cat'\n",
    "        self.pos_enc_d = 128\n",
    "        self.l1_loss = False\n",
    "        self.norm_alpha = self.alpha_norm\n",
    "        self.num_components = 0\n",
    "        self.resample = 7\n",
    "        self.save_norm = True\n",
    "        self.norm_path = os.path.join(data_path, 'norm_coeff')\n",
    "        self.pca_path = os.path.join(data_path, 'pca128_model')\n",
    "        self.load_pca = False\n",
    "        self.compare_model = False\n",
    "        self.channel_idx = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "def control_code(code):\n",
    "    code_dict = {'CTRL_FromServer': 1, 'CTRL_FromClient': 2}\n",
    "    return code_dict.get(code, -1)\n",
    "\n",
    "def data_type(code):\n",
    "    data_dict = {'Data_Info': 1, 'Data_Eeg': 2, 'Data_Events': 3, 'Data_Impedance': 4}\n",
    "    return data_dict.get(code, -1)\n",
    "\n",
    "def request_type(code):\n",
    "    request_dict = {'RequestVersion': 1,\n",
    "                    'RequestChannelInfo': 3,\n",
    "                    'RequestBasicInfoAcq': 6,\n",
    "                    'RequestStreamingStart': 8,\n",
    "                    'RequestStreamingStop': 9}\n",
    "    return request_dict.get(code, -1)\n",
    "\n",
    "def init_header(chanID, code, request, samples, size_body, sizeUn):\n",
    "    # convert each character in chanID to uint8\n",
    "    c_chID = struct.pack('4B', *map(ord, chanID))\n",
    "    w_Code = struct.pack('>H', code)\n",
    "    w_Request = struct.pack('>H', request)\n",
    "    un_Sample = struct.pack('>I', samples)\n",
    "    un_Size = struct.pack('>I', size_body)\n",
    "    un_SizeUn = struct.pack('>I', sizeUn)\n",
    "\n",
    "    return c_chID + w_Code + w_Request + un_Sample + un_Size + un_SizeUn\n",
    "\n",
    "def block_type(code):\n",
    "    block_dict = {'DataTypeFloat32bit': 1,\n",
    "                  'DataTypeFloat32bitZIP': 2,\n",
    "                  'DataTypeEventList': 3}\n",
    "    return block_dict.get(code, -1)\n",
    "\n",
    "def info_type(code):\n",
    "    info_dict = {'InfoType_Version': 1,\n",
    "                 'InfoType_BasicInfo': 2,\n",
    "                 'InfoType_ChannelInfo': 4,\n",
    "                 'InfoType_StatusAmp': 7,\n",
    "                 'InfoType_Time': 9}\n",
    "    return info_dict.get(code, -1)\n",
    "\n",
    "def request_packet(con, packet_size):\n",
    "    count = 0\n",
    "    timeout = 20\n",
    "\n",
    "    while True:\n",
    "        data = con.recv(packet_size, 0)\n",
    "        if data or count == timeout:\n",
    "            break\n",
    "\n",
    "        count += 1\n",
    "        time.sleep(0.2)\n",
    "\n",
    "    return data\n",
    "\n",
    "def client_process_request(con, header, code, request, init):\n",
    "    header_size = len(header)\n",
    "\n",
    "    # send header if streaming start\n",
    "    if not init:\n",
    "        con.send(header)\n",
    "    \n",
    "    # get response header\n",
    "    data = request_packet(con, 20)\n",
    "    \n",
    "    temp_packet_size = 0\n",
    "    count = 0\n",
    "    timeout = 10\n",
    "    synch_packets = 5\n",
    "    data_out = bytearray()\n",
    "    message = {'code': None, 'request': None, 'start_sample': None, 'packet_size': None}\n",
    "    \n",
    "    message['code'] = struct.unpack('>H', data[4:6])[0]\n",
    "    message['request'] = struct.unpack('>H', data[6:8])[0]\n",
    "    message['start_sample'] = struct.unpack('>I', data[8:12])[0]\n",
    "    message['packet_size'] = struct.unpack('>I', data[12:16])[0]\n",
    "    \n",
    "    if message['code'] in code and message['request'] in request:\n",
    "        while temp_packet_size < message['packet_size'] and count < timeout:\n",
    "            data = request_packet(con, message['packet_size'])\n",
    "            temp_packet_size += len(data)\n",
    "            data_out += data\n",
    "            count += 1\n",
    "    else:\n",
    "        while count < synch_packets:\n",
    "            request_packet(con, message['packet_size'])\n",
    "            count += 1\n",
    "    \n",
    "    return data_out, message\n",
    "\n",
    "def client_get_basic_info(con):\n",
    "    basic_info = {}\n",
    "    max_chans = 300\n",
    "    header = init_header(\"CTRL\",\n",
    "                         control_code(\"CTRL_FromClient\"),\n",
    "                         request_type(\"RequestBasicInfoAcq\"),\n",
    "                         0, 0, 0)\n",
    "\n",
    "    basic_info_raw, message = client_process_request(con,\n",
    "                                                     header,\n",
    "                                                     [data_type(\"Data_Info\")],\n",
    "                                                     [info_type(\"InfoType_BasicInfo\")],\n",
    "                                                     0)\n",
    "\n",
    "    size = struct.unpack('<I', basic_info_raw[0:4])[0]\n",
    "    eeg_chan = struct.unpack('<I', basic_info_raw[4:8])[0]\n",
    "    sample_rate = struct.unpack('<I', basic_info_raw[8:12])[0]\n",
    "    data_size = struct.unpack('<I', basic_info_raw[12:16])[0]\n",
    "    allow_client_to_control_amp = struct.unpack('<I', basic_info_raw[16:20])[0]\n",
    "    allow_client_to_control_rec = struct.unpack('<I', basic_info_raw[20:24])[0]\n",
    "    \n",
    "    basic_info = {\n",
    "        'size': size,\n",
    "        'eeg_chan': eeg_chan,\n",
    "        'sample_rate': sample_rate,\n",
    "        'data_size': data_size,\n",
    "        'allow_client_to_control_amp': allow_client_to_control_amp,\n",
    "        'allow_client_to_control_rec': allow_client_to_control_rec\n",
    "    }\n",
    "\n",
    "    return basic_info\n",
    "\n",
    "def request_data_packet(con, basic_info, init=0):\n",
    "    segments = []\n",
    "    offset_event_type = 0\n",
    "    offset_event_latency = offset_event_type + 4\n",
    "    offset_event_start = offset_event_latency + 4\n",
    "    offset_event_end = offset_event_start + 4\n",
    "    offset_event_annotation = offset_event_end + 4\n",
    "\n",
    "    # raw length\n",
    "    event_struct_length = (offset_event_annotation + 520)//8*8\n",
    "\n",
    "    # Protocol variable definitions\n",
    "    data_types   = [data_type('Data_Eeg'), data_type('Data_Events'), data_type('Data_Impedance')]\n",
    "        \n",
    "    block_types  = [block_type('DataTypeFloat32bit'), block_type('DataTypeEventList')]\n",
    "\n",
    "    header = init_header('CTRL',\n",
    "                         control_code('CTRL_FromClient'),\n",
    "                         request_type('RequestStreamingStart'),\n",
    "                         0,0,0)\n",
    "\n",
    "    # get data\n",
    "    data, message = client_process_request(con, header, data_types, block_types, init=init)\n",
    "\n",
    "    # if data packet\n",
    "    if message['code'] == 2: \n",
    "        #receivedSamples = len(data) / (basic_info['data_size'] * basic_info['eeg_chan']) \n",
    "        #print(f\"Received {len(data) / 1000} kBytes, EEG, {receivedSamples} samples, Start sample = {message['startSample']}\")\n",
    "        return data, message\n",
    "\n",
    "    # if event packet\n",
    "    elif message['code'] == 3: \n",
    "        if message['packet_size'] % event_struct_length == 0:\n",
    "            num_events = message['packet_size'] // event_struct_length\n",
    "\n",
    "            if num_events > 0:\n",
    "                event_type = struct.unpack(\n",
    "                    '<I', data[offset_event_type:offset_event_latency])[0]\n",
    "                event_latency = struct.unpack(\n",
    "                    '<I', data[offset_event_latency:offset_event_start])[0]\n",
    "                event_annotation = struct.unpack(\n",
    "                    '<H', data[offset_event_annotation:offset_event_annotation+2])[0]\n",
    "                #print(f\"Event type {eventType}, Latency: {eventLatency}, Annotation: {chr(eventAnnotation)}\")\n",
    "\n",
    "                return {'event_type': event_type,\n",
    "                        'event_latency': event_latency,\n",
    "                        'event_annotation': chr(event_annotation)}, None\n",
    "        else:\n",
    "            print(\"ClientRequestDataPacket failed: unmatching event structure size\")\n",
    "\n",
    "    return data, message\n",
    "\n",
    "def stop_stream(con):\n",
    "    header = init_header('CTRL',\n",
    "                         control_code('CTRL_FromClient'),\n",
    "                         request_type('RequestStreamingStop'),\n",
    "                         0,0,0)\n",
    "    con.send(header)\n",
    "\n",
    "# decode data to numpy\n",
    "def decode_data(data, num_samples, basic_info):\n",
    "    dtype = np.float32 if basic_info['data_size'] == 4 else np.int16\n",
    "    return np.flipud(np.frombuffer(data, dtype=dtype).reshape(num_samples, basic_info[\"eeg_chan\"]))\n",
    "\n",
    "def get_initial_data(sock, basic_info, SR):\n",
    "    data, _ = request_data_packet(sock, basic_info)\n",
    "    num_samples = len(data) // (basic_info[\"data_size\"] * basic_info[\"eeg_chan\"])\n",
    "\n",
    "    segments = []\n",
    "    events = []\n",
    "    train_data = []\n",
    "    train_target = []\n",
    "    start_sample = 0\n",
    "    sample_count = SR + 1\n",
    "\n",
    "    while True:\n",
    "        # start streaming\n",
    "        data, message = request_data_packet(sock, basic_info, 1)\n",
    "\n",
    "        # check if data is dict (event packet)\n",
    "        if isinstance(data, dict):\n",
    "            sample_count = 0\n",
    "            last_event = data['event_type']\n",
    "            last_latency = data['event_latency'] - start_sample\n",
    "            events.append(np.array([last_event, last_latency]))\n",
    "\n",
    "            # if we are at the end of experiment exit loop\n",
    "            if last_event == 7:\n",
    "                break\n",
    "        else:\n",
    "            # set start sample index\n",
    "            if not segments:\n",
    "                start_sample = message['start_sample']\n",
    "\n",
    "            # shape: samples x channels\n",
    "            packet = decode_data(data, num_samples, basic_info)\n",
    "            segments.append(packet)\n",
    "            sample_count += 1\n",
    "\n",
    "        # if we have enough samples to make a trial\n",
    "        if sample_count == SR:\n",
    "            # concatenate last 1 second of samples\n",
    "            trial = np.concatenate(segments[-SR-1:], axis=0)\n",
    "\n",
    "            # calculate latency of last event compared to trial start\n",
    "            latency = int(last_latency - (len(segments) - SR - 1) * 1000 / SR)\n",
    "            train_data.append(trial[latency:latency+1000, :])\n",
    "            train_target.append(last_event - 2)\n",
    "\n",
    "            print(f\"Trial {len(train_data)}: {last_event} at {last_latency} ms\")\n",
    "\n",
    "    stop_stream(sock)\n",
    "\n",
    "    return np.array(segments), np.array(events), np.array(train_data), np.array(train_target)\n",
    "\n",
    "def save_data(args, segments, events, train_data, train_target):\n",
    "    if not os.path.exists(args.result_dir):\n",
    "        os.makedirs(args.result_dir)\n",
    "\n",
    "    np.save(os.path.join(args.result_dir, 'events.npy'), events)\n",
    "    np.save(os.path.join(args.result_dir, 'segments.npy'), segments)\n",
    "    np.save(os.path.join(args.result_dir, 'train_data.npy'), train_data)\n",
    "    np.save(os.path.join(args.result_dir, 'train_target.npy'), train_target)\n",
    "\n",
    "def wavelet_transform(data, args):\n",
    "    # trials, channels, samples\n",
    "    data = data.transpose(0, 2, 1)\n",
    "\n",
    "    f, t, data = stft(data,\n",
    "                      fs=args.sr_data,\n",
    "                      window='hamming',\n",
    "                      nperseg=args.halfwin*2,\n",
    "                      noverlap=args.overlap,\n",
    "                      boundary=None)\n",
    "\n",
    "    # concatenate wavelet coefficients\n",
    "    data = np.concatenate((data.real, data.imag), axis=2)\n",
    "\n",
    "    return data\n",
    "\n",
    "def train_and_predict(args, data, target=None, lda=None, scaler=None, wavelet=False):\n",
    "    init = False\n",
    "    if lda is None:\n",
    "        lda = args.model(args)\n",
    "\n",
    "        if args.scaler is None:\n",
    "            scaler = FunctionTransformer(lambda x: x)\n",
    "        else:\n",
    "            scaler = args.scaler()\n",
    "        init = True\n",
    "\n",
    "    num_chn = len(args.num_channels)\n",
    "\n",
    "    num_trials = data.shape[0]\n",
    "    data = data[:, ::int(1000 / args.sr_data), args.num_channels]\n",
    "    data = data[:, args.sample_rate[0]:args.sample_rate[1], :].reshape(-1, num_chn)\n",
    "\n",
    "    if init:\n",
    "        scaler.fit(data)\n",
    "    \n",
    "    data = scaler.transform(data)\n",
    "    data = data.reshape(num_trials, -1, num_chn)\n",
    "\n",
    "    # compute wavelet transform\n",
    "    if wavelet:\n",
    "        data = wavelet_transform(data, args)\n",
    "    data = data.reshape(num_trials, -1)\n",
    "\n",
    "    if init:\n",
    "        # fit lda model\n",
    "        lda.model.fit(data, target)\n",
    "        print(data.shape)\n",
    "\n",
    "    if target is not None:\n",
    "        # calculate accuracy\n",
    "        pred = lda.model.score(data, target)\n",
    "    else:\n",
    "        # predict probability of test data\n",
    "        pred = lda.model.predict_proba(data)\n",
    "\n",
    "    return pred, lda, scaler\n",
    "\n",
    "def real_time_predict(args, sock, basic_info, lda, scaler):\n",
    "    # start streaming\n",
    "    data, _ = request_data_packet(sock, basic_info)\n",
    "    num_samples = len(data) // (basic_info[\"data_size\"] * basic_info[\"eeg_chan\"])\n",
    "    SR = args.streaming_SR\n",
    "\n",
    "    events = {0: 'hungry',\n",
    "              1: 'tired',\n",
    "              2: 'thirsty',\n",
    "              3: 'toilet',\n",
    "              4: 'pain'}\n",
    "\n",
    "    segments = []\n",
    "    latencies = []\n",
    "    val_data = []\n",
    "    val_target = []\n",
    "    start_sample = 0\n",
    "    num_trials = 0\n",
    "    sample_count = SR + 1\n",
    "    restart = True\n",
    "\n",
    "    while True:\n",
    "        data, message = request_data_packet(sock, basic_info, 1)\n",
    "\n",
    "        # check if data is dict\n",
    "        if isinstance(data, dict):\n",
    "            sample_count = 0\n",
    "            last_event = data['event_type']\n",
    "            last_latency = data['event_latency'] - start_sample\n",
    "        else:\n",
    "            # decode data to numpy\n",
    "            packet = decode_data(data, num_samples, basic_info)\n",
    "        \n",
    "            if restart:\n",
    "                restart = False\n",
    "                start_sample = message['start_sample']\n",
    "\n",
    "            segments.append(packet)\n",
    "            sample_count += 1\n",
    "\n",
    "        # if we have enough samples to make a trial\n",
    "        if sample_count == SR:\n",
    "            num_trials += 1\n",
    "            trial = np.concatenate(segments[-SR-1:], axis=0)\n",
    "\n",
    "            latency = int(last_latency - (len(segments) - SR - 1) * 1000 / SR)\n",
    "            val_data.append(trial[latency:latency+1000, :])\n",
    "            val_target.append(last_event - 2)\n",
    "            print(events[last_event-2])\n",
    "\n",
    "        # make a prediction after 4 trials\n",
    "        if num_trials == 4:\n",
    "            num_trials = 0\n",
    "            stop_stream(sock)\n",
    "        \n",
    "            # make a prediction\n",
    "            probs, _, _ = train_and_predict(args, np.array(val_data), lda=lda, scaler=scaler, wavelet=True)\n",
    "\n",
    "            for p in probs:\n",
    "                # format to 2 decimals and sort by probability\n",
    "                event_probs = {events[j]: round(p[j], 2)*100 for j in events.keys()}\n",
    "\n",
    "                sorted_events = sorted(event_probs.items(),\n",
    "                                       key=lambda item: item[1],\n",
    "                                       reverse=True)\n",
    "                event_probs = {k: int(v) for k, v in sorted_events}\n",
    "                print(event_probs)\n",
    "\n",
    "            # press enter to continue or q to quit\n",
    "            key = input()\n",
    "            if key == 'q':\n",
    "                break\n",
    "            \n",
    "            # restart streaming\n",
    "            restart = True\n",
    "            segments = []\n",
    "            latencies = []\n",
    "            val_data = []\n",
    "            val_target = []\n",
    "\n",
    "            data, _ = request_data_packet(sock, basic_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialise socket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a TCP/IP socket\n",
    "sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "\n",
    "# Bind the socket to the port\n",
    "server_address = ('192.168.0.1', 4455)\n",
    "sock.connect(server_address)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'size': 24,\n",
       " 'eeg_chan': 69,\n",
       " 'sample_rate': 1000,\n",
       " 'data_size': 4,\n",
       " 'allow_client_to_control_amp': 0,\n",
       " 'allow_client_to_control_rec': 0}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basic_info = client_get_basic_info(sock)\n",
    "basic_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial data collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 1: 3 at 9417 ms\n",
      "Trial 2: 3 at 10507 ms\n",
      "Trial 3: 3 at 11596 ms\n",
      "Trial 4: 3 at 12686 ms\n",
      "Trial 5: 6 at 16866 ms\n",
      "Trial 6: 6 at 17956 ms\n",
      "Trial 7: 6 at 19046 ms\n",
      "Trial 8: 6 at 20136 ms\n",
      "Trial 9: 5 at 24495 ms\n",
      "Trial 10: 5 at 25585 ms\n",
      "Trial 11: 5 at 26675 ms\n",
      "Trial 12: 5 at 27765 ms\n",
      "Trial 13: 2 at 32405 ms\n",
      "Trial 14: 2 at 33495 ms\n",
      "Trial 15: 2 at 34584 ms\n",
      "Trial 16: 2 at 35674 ms\n",
      "Trial 17: 4 at 40084 ms\n",
      "Trial 18: 4 at 41174 ms\n",
      "Trial 19: 4 at 42264 ms\n",
      "Trial 20: 4 at 43354 ms\n",
      "Trial 21: 6 at 47563 ms\n",
      "Trial 22: 6 at 48653 ms\n",
      "Trial 23: 6 at 49743 ms\n",
      "Trial 24: 6 at 50833 ms\n",
      "Trial 25: 5 at 55343 ms\n",
      "Trial 26: 5 at 56433 ms\n",
      "Trial 27: 5 at 57523 ms\n",
      "Trial 28: 5 at 58612 ms\n",
      "Trial 29: 3 at 62942 ms\n",
      "Trial 30: 3 at 64032 ms\n",
      "Trial 31: 3 at 65122 ms\n",
      "Trial 32: 3 at 66212 ms\n",
      "Trial 33: 4 at 70621 ms\n",
      "Trial 34: 4 at 71711 ms\n",
      "Trial 35: 4 at 72801 ms\n",
      "Trial 36: 4 at 73891 ms\n",
      "Trial 37: 2 at 78471 ms\n",
      "Trial 38: 2 at 79561 ms\n",
      "Trial 39: 2 at 80651 ms\n",
      "Trial 40: 2 at 81740 ms\n",
      "Trial 41: 3 at 86130 ms\n",
      "Trial 42: 3 at 87220 ms\n",
      "Trial 43: 3 at 88310 ms\n",
      "Trial 44: 3 at 89400 ms\n",
      "Trial 45: 4 at 93749 ms\n",
      "Trial 46: 4 at 94839 ms\n",
      "Trial 47: 4 at 95929 ms\n",
      "Trial 48: 4 at 97019 ms\n",
      "Trial 49: 2 at 101329 ms\n",
      "Trial 50: 2 at 102419 ms\n",
      "Trial 51: 2 at 103509 ms\n",
      "Trial 52: 2 at 104599 ms\n",
      "Trial 53: 6 at 109178 ms\n",
      "Trial 54: 6 at 110268 ms\n",
      "Trial 55: 6 at 111358 ms\n",
      "Trial 56: 6 at 112448 ms\n",
      "Trial 57: 5 at 116727 ms\n",
      "Trial 58: 5 at 117817 ms\n",
      "Trial 59: 5 at 118907 ms\n",
      "Trial 60: 5 at 119997 ms\n",
      "Trial 61: 6 at 124587 ms\n",
      "Trial 62: 6 at 125677 ms\n",
      "Trial 63: 6 at 126767 ms\n",
      "Trial 64: 6 at 127857 ms\n",
      "Trial 65: 2 at 132296 ms\n",
      "Trial 66: 2 at 133386 ms\n",
      "Trial 67: 2 at 134476 ms\n",
      "Trial 68: 2 at 135566 ms\n",
      "Trial 69: 3 at 139776 ms\n",
      "Trial 70: 3 at 140865 ms\n",
      "Trial 71: 3 at 141955 ms\n",
      "Trial 72: 3 at 143045 ms\n",
      "Trial 73: 4 at 147355 ms\n",
      "Trial 74: 4 at 148445 ms\n",
      "Trial 75: 4 at 149535 ms\n",
      "Trial 76: 4 at 150625 ms\n",
      "Trial 77: 5 at 154874 ms\n",
      "Trial 78: 5 at 155964 ms\n",
      "Trial 79: 5 at 157054 ms\n",
      "Trial 80: 5 at 158144 ms\n",
      "Trial 81: 3 at 162404 ms\n",
      "Trial 82: 3 at 163494 ms\n",
      "Trial 83: 3 at 164583 ms\n",
      "Trial 84: 3 at 165673 ms\n",
      "Trial 85: 2 at 170113 ms\n",
      "Trial 86: 2 at 171203 ms\n",
      "Trial 87: 2 at 172293 ms\n",
      "Trial 88: 2 at 173383 ms\n",
      "Trial 89: 6 at 177672 ms\n",
      "Trial 90: 6 at 178762 ms\n",
      "Trial 91: 6 at 179852 ms\n",
      "Trial 92: 6 at 180942 ms\n",
      "Trial 93: 4 at 185392 ms\n",
      "Trial 94: 4 at 186482 ms\n",
      "Trial 95: 4 at 187572 ms\n",
      "Trial 96: 4 at 188661 ms\n",
      "Trial 97: 5 at 193281 ms\n",
      "Trial 98: 5 at 194371 ms\n",
      "Trial 99: 5 at 195461 ms\n",
      "Trial 100: 5 at 196551 ms\n",
      "Trial 101: 5 at 200910 ms\n",
      "Trial 102: 5 at 202000 ms\n",
      "Trial 103: 5 at 203090 ms\n",
      "Trial 104: 5 at 204180 ms\n",
      "Trial 105: 3 at 208550 ms\n",
      "Trial 106: 3 at 209640 ms\n",
      "Trial 107: 3 at 210730 ms\n",
      "Trial 108: 3 at 211819 ms\n",
      "Trial 109: 4 at 216249 ms\n",
      "Trial 110: 4 at 217339 ms\n",
      "Trial 111: 4 at 218429 ms\n",
      "Trial 112: 4 at 219519 ms\n",
      "Trial 113: 2 at 223958 ms\n",
      "Trial 114: 2 at 225048 ms\n",
      "Trial 115: 2 at 226138 ms\n",
      "Trial 116: 2 at 227228 ms\n",
      "Trial 117: 6 at 231748 ms\n",
      "Trial 118: 6 at 232838 ms\n",
      "Trial 119: 6 at 233928 ms\n",
      "Trial 120: 6 at 235018 ms\n",
      "Trial 121: 5 at 239567 ms\n",
      "Trial 122: 5 at 240657 ms\n",
      "Trial 123: 5 at 241747 ms\n",
      "Trial 124: 5 at 242837 ms\n",
      "Trial 125: 3 at 247286 ms\n",
      "Trial 126: 3 at 248376 ms\n",
      "Trial 127: 3 at 249466 ms\n",
      "Trial 128: 3 at 250556 ms\n",
      "Trial 129: 4 at 255126 ms\n",
      "Trial 130: 4 at 256216 ms\n",
      "Trial 131: 4 at 257306 ms\n",
      "Trial 132: 4 at 258396 ms\n",
      "Trial 133: 6 at 262795 ms\n",
      "Trial 134: 6 at 263885 ms\n",
      "Trial 135: 6 at 264975 ms\n",
      "Trial 136: 6 at 266065 ms\n",
      "Trial 137: 2 at 270564 ms\n",
      "Trial 138: 2 at 271654 ms\n",
      "Trial 139: 2 at 272744 ms\n",
      "Trial 140: 2 at 273834 ms\n",
      "Trial 141: 4 at 278244 ms\n",
      "Trial 142: 4 at 279334 ms\n",
      "Trial 143: 4 at 280424 ms\n",
      "Trial 144: 4 at 281514 ms\n",
      "Trial 145: 3 at 286053 ms\n",
      "Trial 146: 3 at 287143 ms\n",
      "Trial 147: 3 at 288233 ms\n",
      "Trial 148: 3 at 289323 ms\n",
      "Trial 149: 5 at 293853 ms\n",
      "Trial 150: 5 at 294942 ms\n",
      "Trial 151: 5 at 296032 ms\n",
      "Trial 152: 5 at 297122 ms\n",
      "Trial 153: 2 at 301642 ms\n",
      "Trial 154: 2 at 302732 ms\n",
      "Trial 155: 2 at 303822 ms\n",
      "Trial 156: 2 at 304912 ms\n",
      "Trial 157: 6 at 309361 ms\n",
      "Trial 158: 6 at 310451 ms\n",
      "Trial 159: 6 at 311541 ms\n",
      "Trial 160: 6 at 312631 ms\n",
      "Trial 161: 3 at 317061 ms\n",
      "Trial 162: 3 at 318150 ms\n",
      "Trial 163: 3 at 319240 ms\n",
      "Trial 164: 3 at 320330 ms\n",
      "Trial 165: 2 at 324920 ms\n",
      "Trial 166: 2 at 326010 ms\n",
      "Trial 167: 2 at 327100 ms\n",
      "Trial 168: 2 at 328190 ms\n",
      "Trial 169: 6 at 332659 ms\n",
      "Trial 170: 6 at 333749 ms\n",
      "Trial 171: 6 at 334839 ms\n",
      "Trial 172: 6 at 335929 ms\n",
      "Trial 173: 5 at 340359 ms\n",
      "Trial 174: 5 at 341448 ms\n",
      "Trial 175: 5 at 342538 ms\n",
      "Trial 176: 5 at 343628 ms\n",
      "Trial 177: 4 at 347918 ms\n",
      "Trial 178: 4 at 349008 ms\n",
      "Trial 179: 4 at 350098 ms\n",
      "Trial 180: 4 at 351188 ms\n",
      "Trial 181: 5 at 355637 ms\n",
      "Trial 182: 5 at 356727 ms\n",
      "Trial 183: 5 at 357817 ms\n",
      "Trial 184: 5 at 358907 ms\n",
      "Trial 185: 3 at 363237 ms\n",
      "Trial 186: 3 at 364327 ms\n",
      "Trial 187: 3 at 365416 ms\n",
      "Trial 188: 3 at 366506 ms\n",
      "Trial 189: 4 at 370876 ms\n",
      "Trial 190: 4 at 371966 ms\n",
      "Trial 191: 4 at 373056 ms\n",
      "Trial 192: 4 at 374146 ms\n",
      "Trial 193: 2 at 378575 ms\n",
      "Trial 194: 2 at 379665 ms\n",
      "Trial 195: 2 at 380755 ms\n",
      "Trial 196: 2 at 381845 ms\n",
      "Trial 197: 6 at 386435 ms\n",
      "Trial 198: 6 at 387525 ms\n",
      "Trial 199: 6 at 388615 ms\n",
      "Trial 200: 6 at 389704 ms\n",
      "Trial 201: 2 at 394164 ms\n",
      "Trial 202: 2 at 395254 ms\n",
      "Trial 203: 2 at 396344 ms\n",
      "Trial 204: 2 at 397434 ms\n",
      "Trial 205: 3 at 401993 ms\n",
      "Trial 206: 3 at 403083 ms\n",
      "Trial 207: 3 at 404173 ms\n",
      "Trial 208: 3 at 405263 ms\n",
      "Trial 209: 6 at 409463 ms\n",
      "Trial 210: 6 at 410553 ms\n",
      "Trial 211: 6 at 411643 ms\n",
      "Trial 212: 6 at 412732 ms\n",
      "Trial 213: 4 at 417172 ms\n",
      "Trial 214: 4 at 418262 ms\n",
      "Trial 215: 4 at 419352 ms\n",
      "Trial 216: 4 at 420442 ms\n",
      "Trial 217: 5 at 424691 ms\n",
      "Trial 218: 5 at 425781 ms\n",
      "Trial 219: 5 at 426871 ms\n",
      "Trial 220: 5 at 427961 ms\n",
      "Trial 221: 2 at 432391 ms\n",
      "Trial 222: 2 at 433481 ms\n",
      "Trial 223: 2 at 434571 ms\n",
      "Trial 224: 2 at 435661 ms\n",
      "Trial 225: 6 at 440170 ms\n",
      "Trial 226: 6 at 441260 ms\n",
      "Trial 227: 6 at 442350 ms\n",
      "Trial 228: 6 at 443440 ms\n",
      "Trial 229: 5 at 447930 ms\n",
      "Trial 230: 5 at 449019 ms\n",
      "Trial 231: 5 at 450109 ms\n",
      "Trial 232: 5 at 451199 ms\n",
      "Trial 233: 3 at 455489 ms\n",
      "Trial 234: 3 at 456579 ms\n",
      "Trial 235: 3 at 457669 ms\n",
      "Trial 236: 3 at 458759 ms\n",
      "Trial 237: 4 at 463198 ms\n",
      "Trial 238: 4 at 464288 ms\n",
      "Trial 239: 4 at 465378 ms\n",
      "Trial 240: 4 at 466468 ms\n",
      "Trial 241: 6 at 470788 ms\n",
      "Trial 242: 6 at 471877 ms\n",
      "Trial 243: 6 at 472967 ms\n",
      "Trial 244: 6 at 474057 ms\n",
      "Trial 245: 3 at 478617 ms\n",
      "Trial 246: 3 at 479707 ms\n",
      "Trial 247: 3 at 480797 ms\n",
      "Trial 248: 3 at 481887 ms\n",
      "Trial 249: 2 at 486266 ms\n",
      "Trial 250: 2 at 487356 ms\n",
      "Trial 251: 2 at 488446 ms\n",
      "Trial 252: 2 at 489536 ms\n",
      "Trial 253: 4 at 493896 ms\n",
      "Trial 254: 4 at 494986 ms\n",
      "Trial 255: 4 at 496075 ms\n",
      "Trial 256: 4 at 497165 ms\n",
      "Trial 257: 5 at 501695 ms\n",
      "Trial 258: 5 at 502785 ms\n",
      "Trial 259: 5 at 503875 ms\n",
      "Trial 260: 5 at 504965 ms\n",
      "Trial 261: 6 at 509484 ms\n",
      "Trial 262: 6 at 510574 ms\n",
      "Trial 263: 6 at 511664 ms\n",
      "Trial 264: 6 at 512754 ms\n",
      "Trial 265: 4 at 517164 ms\n",
      "Trial 266: 4 at 518254 ms\n",
      "Trial 267: 4 at 519343 ms\n",
      "Trial 268: 4 at 520433 ms\n",
      "Trial 269: 2 at 524733 ms\n",
      "Trial 270: 2 at 525823 ms\n",
      "Trial 271: 2 at 526913 ms\n",
      "Trial 272: 2 at 528003 ms\n",
      "Trial 273: 3 at 532492 ms\n",
      "Trial 274: 3 at 533582 ms\n",
      "Trial 275: 3 at 534672 ms\n",
      "Trial 276: 3 at 535762 ms\n",
      "Trial 277: 5 at 540172 ms\n",
      "Trial 278: 5 at 541262 ms\n",
      "Trial 279: 5 at 542352 ms\n",
      "Trial 280: 5 at 543441 ms\n"
     ]
    }
   ],
   "source": [
    "segments, events, train_data, train_target = get_initial_data(sock,\n",
    "                                                              basic_info,\n",
    "                                                              args.streaming_SR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save data\n",
    "#save_data(args, segments, events, train_data, train_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking data\n",
    "cont_segments = segments.transpose(2, 0, 1).reshape(69, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e52fffb5dbe94aa58f762bd47d75713f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7ff27e0815e0>]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib widget\n",
    "plt.plot(cont_segments[[3], 20000:40000].T, linewidth=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1be75b9b979d42bc8a089440f0b1309e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7ff198979070>]"
      ]
     },
     "execution_count": 424,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check evoked responses\n",
    "%matplotlib widget\n",
    "plt.plot(train_data[[200], :, 1].mean(axis=0), linewidth=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "train_data = np.load(os.path.join(args.result_dir, 'train_data.npy'))\n",
    "train_target = np.load(os.path.join(args.result_dir, 'train_target.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(280, 1000, 69)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.num_channels = [0, 1, 2, 64, 65, 66]  # 2, 64, 65 / 0, 1, 2, 64, 65, 66\n",
    "args.halfwin = 10  # 10 / 10\n",
    "args.overlap = 2 # 5 / 2\n",
    "args.C_reg = 0.05 # 0.5\n",
    "args.model = LogisticRegL1 # LDA / LogisticRegL1\n",
    "args.scaler = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(260, 3696)\n",
      "0.9961538461538462\n"
     ]
    }
   ],
   "source": [
    "# train on training split\n",
    "ntrials = 260\n",
    "acc, lda, scaler = train_and_predict(\n",
    "    args, train_data[:ntrials], train_target[:ntrials], wavelet=True)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n"
     ]
    }
   ],
   "source": [
    "# test on validation split\n",
    "acc, _, _ = train_and_predict(\n",
    "    args, train_data[ntrials:], train_target[ntrials:], lda=lda, scaler=scaler, wavelet=True)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimize hyperparams\n",
    "C_reg = [0.01, 0.05, 0.1, 1, 2, 10]\n",
    "halfwin = [5, 7, 9, 11, 13, 15]\n",
    "overlap = [1, 3, 5, 7]\n",
    "num_channels = [[0, 1, 2, 64, 65, 66], [2, 64, 65], [0, 1, 2], [64, 65, 66]]\n",
    "scalers = [None, StandardScaler, RobustScaler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimize hyperparams\n",
    "C_reg = [0.01, 0.05, 0.1, 1, 2, 10]\n",
    "halfwin = [7]\n",
    "overlap = [7]\n",
    "num_channels = [[0, 1, 2, 64, 65, 66]]\n",
    "pca_comps = [2, 5, 8]\n",
    "scalers = [None]\n",
    "wavelet = [False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n",
      "[CV 1/5] END halfwin=7, num_channels=[0, 1, 2, 64, 65, 66], overlap=7, pca_comps=2, scaler=None, wavelet=False;, score=0.288 total time=   0.1s\n",
      "[CV 2/5] END halfwin=7, num_channels=[0, 1, 2, 64, 65, 66], overlap=7, pca_comps=2, scaler=None, wavelet=False;, score=0.212 total time=   0.2s\n",
      "[CV 5/5] END halfwin=7, num_channels=[0, 1, 2, 64, 65, 66], overlap=7, pca_comps=2, scaler=None, wavelet=False;, score=0.096 total time=   0.2s\n",
      "[CV 3/5] END halfwin=7, num_channels=[0, 1, 2, 64, 65, 66], overlap=7, pca_comps=2, scaler=None, wavelet=False;, score=0.192 total time=   0.2s\n",
      "[CV 4/5] END halfwin=7, num_channels=[0, 1, 2, 64, 65, 66], overlap=7, pca_comps=2, scaler=None, wavelet=False;, score=0.173 total time=   0.2s\n",
      "[CV 2/5] END halfwin=7, num_channels=[0, 1, 2, 64, 65, 66], overlap=7, pca_comps=5, scaler=None, wavelet=False;, score=0.231 total time=   0.2s\n",
      "[CV 1/5] END halfwin=7, num_channels=[0, 1, 2, 64, 65, 66], overlap=7, pca_comps=5, scaler=None, wavelet=False;, score=0.212 total time=   0.2s\n",
      "[CV 3/5] END halfwin=7, num_channels=[0, 1, 2, 64, 65, 66], overlap=7, pca_comps=5, scaler=None, wavelet=False;, score=0.173 total time=   0.2s\n",
      "[CV 4/5] END halfwin=7, num_channels=[0, 1, 2, 64, 65, 66], overlap=7, pca_comps=5, scaler=None, wavelet=False;, score=0.250 total time=   0.2s\n",
      "[CV 1/5] END halfwin=7, num_channels=[0, 1, 2, 64, 65, 66], overlap=7, pca_comps=8, scaler=None, wavelet=False;, score=0.327 total time=   0.2s\n",
      "[CV 5/5] END halfwin=7, num_channels=[0, 1, 2, 64, 65, 66], overlap=7, pca_comps=5, scaler=None, wavelet=False;, score=0.269 total time=   0.2s\n",
      "[CV 2/5] END halfwin=7, num_channels=[0, 1, 2, 64, 65, 66], overlap=7, pca_comps=8, scaler=None, wavelet=False;, score=0.250 total time=   0.2s\n",
      "[CV 3/5] END halfwin=7, num_channels=[0, 1, 2, 64, 65, 66], overlap=7, pca_comps=8, scaler=None, wavelet=False;, score=0.327 total time=   0.1s\n",
      "[CV 4/5] END halfwin=7, num_channels=[0, 1, 2, 64, 65, 66], overlap=7, pca_comps=8, scaler=None, wavelet=False;, score=0.288 total time=   0.1s\n",
      "[CV 5/5] END halfwin=7, num_channels=[0, 1, 2, 64, 65, 66], overlap=7, pca_comps=8, scaler=None, wavelet=False;, score=0.346 total time=   0.1s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(estimator=LDATune(), n_jobs=-1,\n",
       "             param_grid={'halfwin': [7],\n",
       "                         'num_channels': [[0, 1, 2, 64, 65, 66]],\n",
       "                         'overlap': [7], 'pca_comps': [2, 5, 8],\n",
       "                         'scaler': [None], 'wavelet': [False]},\n",
       "             scoring='accuracy', verbose=3)"
      ]
     },
     "execution_count": 527,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gridCV = GridSearchCV(estimator=LDATune(),\n",
    "                      param_grid={'halfwin': halfwin,\n",
    "                                  'overlap': overlap,\n",
    "                                  'num_channels': num_channels,\n",
    "                                  'scaler': scalers,\n",
    "                                  'pca_comps': pca_comps,\n",
    "                                  'wavelet': wavelet},\n",
    "                      scoring='accuracy',\n",
    "                      n_jobs=-1,\n",
    "                      verbose=3)\n",
    "\n",
    "gridCV.fit(train_data[:ntrials], train_target[:ntrials])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LDATune(overlap=7, pca_comps=8, wavelet=False)\n",
      "0.30769230769230765\n"
     ]
    }
   ],
   "source": [
    "print(gridCV.best_estimator_)\n",
    "print(gridCV.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2"
      ]
     },
     "execution_count": 529,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model = LDATune(halfwin=7, overlap=7, pca_comps=10, wavelet=False)\n",
    "best_model.fit(train_data[:ntrials], train_target[:ntrials])\n",
    "best_model.score(train_data[ntrials:], train_target[ntrials:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Closed-loop prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pain\n",
      "pain\n",
      "pain\n",
      "pain\n",
      "{'toilet': 44, 'thirsty': 40, 'hungry': 16, 'tired': 0, 'pain': 0}\n",
      "{'toilet': 49, 'pain': 49, 'thirsty': 2, 'hungry': 0, 'tired': 0}\n",
      "{'hungry': 92, 'pain': 6, 'thirsty': 1, 'toilet': 1, 'tired': 0}\n",
      "{'toilet': 93, 'pain': 5, 'tired': 1, 'thirsty': 1, 'hungry': 0}\n",
      "hungry\n",
      "hungry\n",
      "hungry\n",
      "hungry\n",
      "{'tired': 32, 'thirsty': 31, 'toilet': 31, 'pain': 5, 'hungry': 1}\n",
      "{'hungry': 96, 'tired': 3, 'thirsty': 0, 'toilet': 0, 'pain': 0}\n",
      "{'tired': 99, 'hungry': 0, 'thirsty': 0, 'toilet': 0, 'pain': 0}\n",
      "{'toilet': 59, 'tired': 39, 'pain': 2, 'hungry': 0, 'thirsty': 0}\n",
      "toilet\n",
      "toilet\n",
      "toilet\n",
      "toilet\n",
      "{'thirsty': 73, 'pain': 23, 'hungry': 1, 'tired': 1, 'toilet': 1}\n",
      "{'thirsty': 41, 'tired': 40, 'toilet': 14, 'hungry': 6, 'pain': 0}\n",
      "{'hungry': 79, 'pain': 18, 'thirsty': 3, 'tired': 0, 'toilet': 0}\n",
      "{'thirsty': 44, 'tired': 26, 'pain': 25, 'toilet': 5, 'hungry': 0}\n",
      "tired\n",
      "tired\n",
      "tired\n",
      "tired\n",
      "{'thirsty': 63, 'tired': 33, 'pain': 5, 'hungry': 0, 'toilet': 0}\n",
      "{'toilet': 45, 'tired': 22, 'thirsty': 18, 'pain': 13, 'hungry': 2}\n",
      "{'thirsty': 50, 'hungry': 26, 'toilet': 24, 'tired': 0, 'pain': 0}\n",
      "{'toilet': 98, 'tired': 1, 'pain': 1, 'hungry': 0, 'thirsty': 0}\n"
     ]
    }
   ],
   "source": [
    "real_time_predict(args, sock, basic_info, lda, scaler)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('main')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3818df9f0fda158f80d2404c4b51c3eecd7a0fbc4149212a6ea8c0c98c2a56f5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
